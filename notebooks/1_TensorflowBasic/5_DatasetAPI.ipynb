{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset API\n",
    "---\n",
    "In this notebook, I'll describe how to use the Dataset API in your training.  \n",
    "First of all, I'll show you the implementation of batch generator using numpy.  \n",
    "After that, I show you some examples of batches using the Dataset API.\n",
    "\n",
    "## Table of contents\n",
    "---\n",
    "- Batch generator using numpy\n",
    "    - mnist example\n",
    "- Batch generator using Dataset API\n",
    "  - tf.data.Dataset\n",
    "  - tf.data.Iterator\n",
    "  - mnist example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__packages:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  1.13.1\n",
      "numpy version:  1.16.2\n",
      "scikit learn version:  0.20.3\n",
      "matplotlib version:  3.0.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"tensorflow version: \", tf.__version__)\n",
    "print(\"numpy version: \", np.__version__)\n",
    "print(\"scikit learn version: \", sklearn.__version__)\n",
    "print(\"matplotlib version: \", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generator using numpy\n",
    "---\n",
    "### Load mnist dataset as image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_images(file_path):\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1,784)\n",
    "    return data.reshape(data.shape[0], 28, 28, 1)\n",
    "\n",
    "def load_mnist_labels(file_path):\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './../../data/mnist/'\n",
    "\n",
    "filenames = {\n",
    "    'test_images':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_labels':'t10k-labels-idx1-ubyte.gz',\n",
    "    'train_images':'train-images-idx3-ubyte.gz',\n",
    "    'train_labels':'train-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "train_images = load_mnist_images(dataset_dir + filenames['train_images'])\n",
    "train_labels = load_mnist_labels(dataset_dir + filenames['train_labels'])\n",
    "# split into train/validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_test = load_mnist_images(dataset_dir + filenames['test_images'])\n",
    "y_test = load_mnist_labels(dataset_dir + filenames['test_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.\n",
    "X_val = X_val / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categoly: 3, n_data: 4\n"
     ]
    }
   ],
   "source": [
    "def get_one_hot_labels(categolical_labels):\n",
    "    '''Convert categolical labels into one-hot labels\n",
    "    # Arguments\n",
    "        categolical_labels: Categolical labels which start from zero.\n",
    "    '''\n",
    "    n_categoly = len(np.unique(categolical_labels))\n",
    "    n_data = len(categolical_labels)\n",
    "    print('n_categoly: {}, n_data: {}'.format(n_categoly, n_data))\n",
    "    one_hot_labels = np.zeros((n_data, n_categoly), dtype=float)\n",
    "    for idx in range(n_data):\n",
    "        label = categolical_labels[idx]\n",
    "        one_hot_labels[idx,label] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "# Test code\n",
    "test_input = np.array([0,0,1,2])\n",
    "expected = np.array([[1.,0.,0.],\n",
    "                     [1.,0.,0.],\n",
    "                     [0.,1.,0.],\n",
    "                     [0.,0.,1.]])\n",
    "actual = get_one_hot_labels(test_input)\n",
    "assert (expected == actual).all(), \"Test Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categoly: 10, n_data: 48000\n",
      "n_categoly: 10, n_data: 12000\n",
      "n_categoly: 10, n_data: 10000\n"
     ]
    }
   ],
   "source": [
    "y_train = get_one_hot_labels(y_train)\n",
    "y_val = get_one_hot_labels(y_val)\n",
    "y_test = get_one_hot_labels(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=32):\n",
    "    '''Create a generator that returns batch of size batch_size\n",
    "    # Arguments\n",
    "        X: Numpy array you want to make batch from\n",
    "        y: labels of X\n",
    "        batch_size: Batch size, the number of data per batch\n",
    "    # Returns\n",
    "        generator which returns (X_batch, y_batch) \n",
    "    '''\n",
    "    \n",
    "    assert X.shape[0] == y.shape[0], \"Data shape does not match!!\"\n",
    "    \n",
    "    n_data = X.shape[0]\n",
    "    for idx in range(0, n_data, batch_size):\n",
    "        # Over index is cared in this version of numpy\n",
    "        X_batch = X[idx:idx+batch_size]\n",
    "        y_batch = y[idx:idx+batch_size]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "# define generator\n",
    "train_bach_generator = batch_generator(X_train, y_train, 64)\n",
    "\n",
    "print('type: ', type(train_bach_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 1)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# One shot data batch\n",
    "X_, y_ = next(train_bach_generator)\n",
    "\n",
    "print(X_.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat getting batches\n",
    "for X_, y_ in train_bach_generator:\n",
    "    # Some processing\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generator using Dataset API\n",
    "---\n",
    "### Pipline\n",
    "- Load data into the Dataset object.\n",
    "- Create iterator from the Dataset object.\n",
    "- Get data and feed it into the network.\n",
    "\n",
    "### Main components\n",
    "`tf.data.Dataset`\n",
    "- Create a source.\n",
    "- Applying a transformation.\n",
    "\n",
    "`tf.data.Iterator`\n",
    "- Create a iterator from the Dataset object.\n",
    "- Applying a batch generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.data.Dataset`\n",
    "---\n",
    "__Notifications:  \n",
    "In this section, I describe dataset operations using eager_execution so as to be able to understand the outputs easily. If you want to run the code cells of next section namede tf.data.Iterator which uses tf.Session(), please do not run this section.__\n",
    "\n",
    "- https://www.tensorflow.org/guide/datasets\n",
    "- https://www.tensorflow#org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a source\n",
    "#### - `from_tensors`\n",
    "\n",
    "Creates a Dataset with a single element.\n",
    "\n",
    "> Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in [this guide](https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
      "<dtype: 'int32'>\n",
      "(10,)\n",
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 1D vector\n",
    "X_sample = tf.data.Dataset.from_tensors(tf.range(0, 10))\n",
    "print(type(X_sample))\n",
    "print(X_sample.output_types)\n",
    "print(X_sample.output_shapes)\n",
    "\n",
    "# We can treat dataset objects as iterator in eager mode\n",
    "for d in X_sample:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
      "<dtype: 'float32'>\n",
      "(2, 3)\n",
      "tf.Tensor(\n",
      "[[0.94202423 0.8030591  0.5831975 ]\n",
      " [0.17291617 0.01673031 0.54511225]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 2D matrix\n",
    "# Input matrix will be teated as single element.\n",
    "X_sample = tf.data.Dataset.from_tensors(tf.random_uniform((2,3)))\n",
    "print(type(X_sample))\n",
    "print(X_sample.output_types)\n",
    "print(X_sample.output_shapes)\n",
    "\n",
    "for d in X_sample:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - `from_tensor_slices`\n",
    "Creates a Dataset whose elements are slices of the given tensors.  \n",
    "> Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in [this guide](https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
      "<dtype: 'int32'>\n",
      "()\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 1D vector\n",
    "X_sample = tf.data.Dataset.from_tensor_slices(tf.range(0, 10))\n",
    "print(type(X_sample))\n",
    "print(X_sample.output_types)\n",
    "print(X_sample.output_shapes)\n",
    "\n",
    "for d in X_sample:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
      "<dtype: 'float32'>\n",
      "(3,)\n",
      "tf.Tensor([0.3265065 0.7801666 0.5116267], shape=(3,), dtype=float32)\n",
      "tf.Tensor([0.5576527  0.57961714 0.7255517 ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 2D matrix\n",
    "# Input matrix will be teated as single element.\n",
    "X_sample = tf.data.Dataset.from_tensor_slices(tf.random_uniform((2,3)))\n",
    "print(type(X_sample))\n",
    "print(X_sample.output_types)\n",
    "print(X_sample.output_shapes)\n",
    "\n",
    "for d in X_sample:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\- zip Dataset objects__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(10)]), TensorShape([Dimension(1)]))\n",
      "(<tf.Tensor: id=117, shape=(10,), dtype=float32, numpy=\n",
      "array([0.54385114, 0.79987025, 0.966998  , 0.5304023 , 0.53878844,\n",
      "       0.14456284, 0.31475198, 0.8985603 , 0.03431952, 0.33608198],\n",
      "      dtype=float32)>, <tf.Tensor: id=118, shape=(1,), dtype=float32, numpy=array([0.4512118], dtype=float32)>)\n",
      "(<tf.Tensor: id=121, shape=(10,), dtype=float32, numpy=\n",
      "array([0.29774928, 0.25932002, 0.42823505, 0.20841694, 0.46167576,\n",
      "       0.18783677, 0.28852165, 0.73213303, 0.15576804, 0.01828802],\n",
      "      dtype=float32)>, <tf.Tensor: id=122, shape=(1,), dtype=float32, numpy=array([0.08884335], dtype=float32)>)\n",
      "(<tf.Tensor: id=125, shape=(10,), dtype=float32, numpy=\n",
      "array([0.5363462 , 0.50935555, 0.10325265, 0.03271317, 0.1840074 ,\n",
      "       0.7609285 , 0.03748512, 0.5316086 , 0.5329342 , 0.24820483],\n",
      "      dtype=float32)>, <tf.Tensor: id=126, shape=(1,), dtype=float32, numpy=array([0.26634085], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# Input two tensor\n",
    "X_sample = tf.data.Dataset.from_tensor_slices(tf.random_uniform((3,10)))\n",
    "y_sample = tf.data.Dataset.from_tensor_slices(tf.random_uniform((3,1)))\n",
    "dataset_sample = tf.data.Dataset.zip((X_sample, y_sample))\n",
    "\n",
    "print(type(dataset_sample))\n",
    "print(dataset_sample.output_types)\n",
    "print(dataset_sample.output_shapes)\n",
    "\n",
    "for d in dataset_sample:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\- Give names to each component of an element__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n",
      "tf.Tensor(0.033620358, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9896467, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17672813, shape=(), dtype=float32)\n",
      "tf.Tensor(0.24519813, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)\n",
    "print(dataset.output_shapes)\n",
    "\n",
    "# We can treat a tensor data like a dictionary\n",
    "for d in dataset:\n",
    "    print(d['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying a transformation\n",
    "#### - `map(map_func, num_parallel_calls=None)`\n",
    "- This function applies elementwise transformation\n",
    "- We can use lambda equations in addition to normal functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before mapping\n",
      "tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\n",
      "After mapping\n",
      "tf.Tensor([2 3 4 5 6], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sample_const = tf.constant([1,2,3,4,5])\n",
    "sample_dataset = tf.data.Dataset.from_tensors(sample_const)\n",
    "print('Before mapping')\n",
    "for d in sample_dataset:\n",
    "    print(d)\n",
    "\n",
    "sample_dataset = sample_dataset.map(lambda x: x+1)\n",
    "print('After mapping')\n",
    "for d in sample_dataset:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - `shuffle`\n",
    "> This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([7 8 9], shape=(3,), dtype=int32)\n",
      "tf.Tensor([10 11 12], shape=(3,), dtype=int32)\n",
      "tf.Tensor([13 14 15], shape=(3,), dtype=int32)\n",
      "tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sample_const = tf.data.Dataset.from_tensor_slices(tf.constant([[1,2,3],\n",
    "                                                               [4,5,6],\n",
    "                                                               [7,8,9],\n",
    "                                                               [10,11,12],\n",
    "                                                               [13,14,15]])).shuffle(buffer_size=5)\n",
    "for d in sample_const:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - `repeat`\n",
    "> Repeats this dataset count times.  \n",
    "The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n",
      "tf.Tensor([7 8 9], shape=(3,), dtype=int32)\n",
      "tf.Tensor([10 11 12], shape=(3,), dtype=int32)\n",
      "tf.Tensor([13 14 15], shape=(3,), dtype=int32)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n",
      "tf.Tensor([7 8 9], shape=(3,), dtype=int32)\n",
      "tf.Tensor([10 11 12], shape=(3,), dtype=int32)\n",
      "tf.Tensor([13 14 15], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sample_const = tf.data.Dataset.from_tensor_slices(tf.constant([[1,2,3],\n",
    "                                                               [4,5,6],\n",
    "                                                               [7,8,9],\n",
    "                                                               [10,11,12],\n",
    "                                                               [13,14,15]])).repeat(count=2)\n",
    "for d in sample_const:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - `batch`\n",
    "> Combines consecutive elements of this dataset into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 7  8  9]\n",
      " [10 11 12]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor([[13 14 15]], shape=(1, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sample_const = tf.data.Dataset.from_tensor_slices(tf.constant([[1,2,3],\n",
    "                                                               [4,5,6],\n",
    "                                                               [7,8,9],\n",
    "                                                               [10,11,12],\n",
    "                                                               [13,14,15]])).batch(2)\n",
    "for d in sample_const:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.data.Iterator`\n",
    "---\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Iterator\n",
    "\n",
    "__Notifications:  \n",
    "If you want to run the code cells of this section, please do not run the previous section which uses eager_execution.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a iterator from the Dataset object / Applying a batch generation\n",
    "#### - `make_one_shot_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "sample_dataset = tf.data.Dataset.from_tensor_slices(tf.range(20))\n",
    "sample_iter = sample_dataset.make_one_shot_iterator()\n",
    "# define ops\n",
    "next_data = sample_iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for ii in range(20):\n",
    "        print(sess.run(next_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - `make_initializable_iterator`\n",
    "> An initializable iterator requires you to run an explicit iterator.initializer operation before using it. In exchange for this inconvenience, it enables you to parameterize the definition of the dataset, using one or more tf.placeholder() tensors that can be fed when you initialize the iterator.\n",
    "\n",
    "Pipeline:  \n",
    "- Define generic iterator.\n",
    "- Initialize iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "\n",
    "# Create iterator\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# Define ops\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Initialize an iterator over a dataset with 10 elements.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "    for i in range(10):\n",
    "        value = sess.run(next_element)\n",
    "        print(value)\n",
    "        assert i == value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the same iterator over a dataset with 100 elements.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "    for i in range(100):\n",
    "        value = sess.run(next_element)\n",
    "        assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - `from_structure`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training dataset and validation dataset (same structure)\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generic iterator\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "# Define ops of the generic iterator object\n",
    "next_data = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ops for initializing iterators to get concreate iterator\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 20 epochs in which the training dataset is traversed, followed by the\n",
    "# validation dataset.\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(20):\n",
    "        # Initialize an iterator over the training dataset.\n",
    "        sess.run(training_init_op)\n",
    "        for _ in range(100):\n",
    "            sess.run(next_data)\n",
    "\n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(validation_init_op)\n",
    "        for _ in range(50):\n",
    "            sess.run(next_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Build it together) Create batch generator using Dataset API\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist\n",
    "train_images = load_mnist_images(dataset_dir + filenames['train_images'])\n",
    "train_labels = load_mnist_labels(dataset_dir + filenames['train_labels'])\n",
    "\n",
    "# split into train/validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_test = load_mnist_images(dataset_dir + filenames['test_images'])\n",
    "y_test = load_mnist_labels(dataset_dir + filenames['test_labels'])\n",
    "\n",
    "X_train = np.float32(X_train)\n",
    "X_val = np.float32(X_val)\n",
    "X_test = np.float32(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "norm = tf.constant(255.0)\n",
    "\n",
    "# Build source dataset for training\n",
    "X_train_dataset = tf.data.Dataset.from_tensor_slices(X_train).map(lambda x : tf.divide(x, norm))\n",
    "y_train_dataset = tf.data.Dataset.from_tensor_slices(y_train).map(lambda x : tf.one_hot(x, 10))\n",
    "train_dataset = tf.data.Dataset.zip((X_train_dataset, y_train_dataset)).batch(batch_size)\n",
    "\n",
    "# Build source dataset for validation\n",
    "X_valid_dataset = tf.data.Dataset.from_tensor_slices(X_val).map(lambda x : tf.divide(x, norm))\n",
    "y_valid_dataset = tf.data.Dataset.from_tensor_slices(y_val).map(lambda x : tf.one_hot(x, 10))\n",
    "validation_dataset = tf.data.Dataset.zip((X_valid_dataset, y_valid_dataset)).batch(batch_size)\n",
    "\n",
    "# Make iterator from dataset structure\n",
    "dataset_iter = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "# Define ops for batch generation\n",
    "next_data = dataset_iter.get_next()\n",
    "\n",
    "# Define initialize ops\n",
    "train_iter_init = dataset_iter.make_initializer(train_dataset)\n",
    "validation_iter_init = dataset_iter.make_initializer(validation_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        # training\n",
    "        sess.run(train_iter_init)\n",
    "        while True:\n",
    "            try:\n",
    "                train_batch = sess.run(next_data)\n",
    "                # Add training ops here\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        # validation\n",
    "        sess.run(validation_iter_init)\n",
    "        while True:\n",
    "            try:\n",
    "                validation_batch = sess.run(next_data)\n",
    "                # Add validation ops here\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
